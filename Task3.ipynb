{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Step 1: Load customer and transaction data\n",
    "customers = pd.read_csv('Customers.csv', encoding='utf-8')\n",
    "transactions = pd.read_csv('Transactions.csv', encoding='utf-8')\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Aggregate transaction data by CustomerID: Calculate total spending and product diversity (unique products bought)\n",
    "transaction_summary = transactions.groupby('CustomerID').agg({\n",
    "    'Amount': 'sum',  # Total spending for each customer\n",
    "    'ProductID': 'nunique',  # Number of unique products bought\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Merge customer profile data with transaction data\n",
    "# Assuming the 'Customers.csv' contains CustomerID, Age, Gender, and Location\n",
    "customer_profiles = pd.merge(customers[['CustomerID', 'Age']], transaction_summary, on='CustomerID')\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "# Encoding categorical features like Gender and Location\n",
    "customers_encoded = pd.get_dummies(customers[['Gender', 'Location']])\n",
    "\n",
    "# Combine encoded features with numeric features (Age, Amount, ProductID)\n",
    "customer_profiles = pd.concat([customer_profiles, customers_encoded], axis=1)\n",
    "\n",
    "# Step 5: Data Standardization\n",
    "# Standardizing the data to bring all features to the same scale\n",
    "scaler = StandardScaler()\n",
    "scaled_profiles = scaler.fit_transform(customer_profiles.drop('CustomerID', axis=1))\n",
    "\n",
    "# Step 6: Clustering with KMeans\n",
    "# We will perform KMeans clustering for different values of k (2 to 10 clusters)\n",
    "db_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "# Iterate over the range of clusters\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_profiles)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Calculate Davies-Bouldin Index for the current clustering\n",
    "    db_index = davies_bouldin_score(scaled_profiles, cluster_labels)\n",
    "    db_scores.append(db_index)\n",
    "\n",
    "# Step 7: Visualize Davies-Bouldin Index for each k\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, db_scores, marker='o', linestyle='--', color='b')\n",
    "plt.title(\"Davies-Bouldin Index for Different Numbers of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Davies-Bouldin Index\")\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Select the best k based on DB Index (lower is better)\n",
    "best_k = k_range[np.argmin(db_scores)]\n",
    "print(f\"Optimal number of clusters based on DB Index: {best_k}\")\n",
    "\n",
    "# Step 9: Perform clustering with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(scaled_profiles)\n",
    "\n",
    "# Step 10: Visualizing the clusters using PCA (2D projection)\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(scaled_profiles)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=pca_components[:, 0], y=pca_components[:, 1], hue=cluster_labels, palette='viridis', s=100, edgecolor='k')\n",
    "plt.title(f\"Customer Segmentation with {best_k} Clusters\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Saving the clustering results\n",
    "customer_profiles['Cluster'] = cluster_labels\n",
    "customer_profiles['CustomerID'] = customers['CustomerID']\n",
    "customer_profiles.to_csv('Customer_Segmentation_Results.csv', index=False)\n",
    "\n",
    "# Step 12: Print a sample of the clustered data\n",
    "customer_profiles.head()\n",
    "\n",
    "# Step 13: Additional Metrics (Optional)\n",
    "# Calculate other metrics like silhouette score (optional)\n",
    "from sklearn.metrics import silhouette_score\n",
    "sil_score = silhouette_score(scaled_profiles, cluster_labels)\n",
    "print(f\"Silhouette Score for the clustering: {sil_score:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
